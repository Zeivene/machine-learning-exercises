{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise - NLP_and_RNN_exercise.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq5dzMnlU5g4"
      },
      "source": [
        "# NLP and RNN\n",
        "In this notebook we will check our understanding of the concepts learned in NLP and RNN.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1ZtBJcNXO-BQhK86joluZYzN7b1GHYLLs)\n",
        "\n",
        "To summarize NLP or Natural Language Processing is:\n",
        "\n",
        "* Computer manipulation of natural languages.\n",
        "* Set of methods/algorithms for making natural language accessible to computer.\n",
        "\n",
        "\n",
        "The image below summarizes the basic steps involved in any NLP task:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1PNXHplPzlWnrddeh5oPX6ZuJSfgnuHCa)\n",
        "\n",
        "\n",
        "\n",
        "There are 5 exercises in total and an optional exercise. To answer some of the exercises (3, 4 and 5) you will be required to write a code from scratch in the code cells containing:\n",
        "\n",
        "`# write your code here`\n",
        "\n",
        "\n",
        "Before starting make sure you are using the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0nYAP8Gg8Rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0070d88e-206a-4a44-89e2-f9c6eb541490"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug 20 19:06:01 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFW1f15r0-Nj"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "\n",
        "We will use TensorFlow Keras Tokenizer to tokenize our text. As per the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer):\n",
        "\n",
        "“This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf.”\n",
        "\n",
        "\n",
        "There are many functions that we can use, but below we will be using these two functions to train the tokenizer to our text data and convert given text to tokens:\n",
        "\n",
        "* `fit_on_texts`: Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary “word_index” such that every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word.\n",
        "\n",
        "* `texts_to_sequences` Transforms each text in texts to a sequence of integers. It takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIlcRTXsQ1tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d749006-3b53-48fb-c57d-ba52e6733e89"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "t  = Tokenizer()\n",
        "fit_text = [\"In science, you can say things that seem crazy, but in the long run, they can turn out to be right\"]\n",
        "t.fit_on_texts(fit_text)\n",
        "\n",
        "\n",
        "test_text1 = \"I would like to take a right turn\"\n",
        "test_text2 = \"That man is crazy\"\n",
        "sequences = t.texts_to_sequences([test_text1, test_text2])\n",
        "\n",
        "print('sequences : ',sequences,'\\n')\n",
        "\n",
        "print('word_index : ',t.word_index)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequences :  [[17, 19, 15], [7, 9]] \n",
            "\n",
            "word_index :  {'in': 1, 'can': 2, 'science': 3, 'you': 4, 'say': 5, 'things': 6, 'that': 7, 'seem': 8, 'crazy': 9, 'but': 10, 'the': 11, 'long': 12, 'run': 13, 'they': 14, 'turn': 15, 'out': 16, 'to': 17, 'be': 18, 'right': 19}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvdb0TqFQ8fm"
      },
      "source": [
        "#### Exercise 1\n",
        "In the code above we tokenize two sentences:\n",
        "* \"I would like to take a right turn\"\n",
        "* \"That man is crazy\"\n",
        "\n",
        "a. What is the tokenized version of these sentences?\n",
        "\n",
        "b. The first sentence has 8 words, and second sentence has 4 words, however the tokenized version has 3 and 2 integers respectively for them. Why is it so?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCd4YxPfTZwz"
      },
      "source": [
        "**Answer 1:**\n",
        "\n",
        "**a.** \"I would like to take a right turn\" = **[17, 19, 15]**, \"That man is crazy\" = **[7, 9]**.\n",
        "\n",
        "**b.** Only words that are included in the **fit_text** data are identified. Since these two sentences contain words that are unknown to the **word_index** dictionary, they are not recognised."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOC-X3I4SyDo"
      },
      "source": [
        "## Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI3bQ3Xkn9JQ"
      },
      "source": [
        "####Exercise 2\n",
        "In the class we learned about embeddings, let us explore them a little more. Kindly go to the site [Embeddings Projector](http://projector.tensorflow.org). Play around a bit and answer the following questions:\n",
        "\n",
        "1.  For the word 'fantastic' list the five nearest neighbours, when using `Word2Vec 10K` embedding.\n",
        "2. Repeat the exercise by changing the embeddings to `Word2Vec All`.\n",
        "\n",
        "Reflect on the result. How do you think the world `fantastic` is related to its five nearest neighbours?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miPD6Wu1WPPZ"
      },
      "source": [
        "**Answer 2:**\n",
        "\n",
        "**1.** Word2Vec 10K: **Marvel; Spider; Amazing; Strange; Weird**\n",
        "\n",
        "**2.** Word2Vec All: **Fantastical; Fantastically**\n",
        "\n",
        "The word **'fantastic'** is semantically related to some of its neighbouring words but not all. Although it's nearest neighbour **'spider'** and **'marvel'** are nouns compared to **'fantastic'** (an adjective), it might be derived from related category brackets which are not related in terms of meanings **(e.g. 'Fantastic' Four & 'Spider'-Man is are frequently mentioned in the 'Marvel' Comics and their labels are considered as nouns)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRtE9HduWWoo"
      },
      "source": [
        "## Word Similarities\n",
        "Let us now train Word2Vec model on text8 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSlDQfKoR5zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3421521-6e47-4f38-a2d9-6f6966566ab0"
      },
      "source": [
        "!mkdir data\n",
        "\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "info = api.info(\"text8\")\n",
        "assert(len(info) > 0)\n",
        "\n",
        "dataset = api.load(\"text8\")  # download and load text 8  dataset\n",
        "model = Word2Vec(dataset) # we create an embedding using Word2vec model for this data\n",
        "\n",
        "model.save(\"data/text8-word2vec.bin\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4qqaBk7XLGu"
      },
      "source": [
        "#### Load the saved model as KeyedVector to save space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W26H--UnW3cL"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load(\"data/text8-word2vec.bin\")  # Help in saving memory by shedding the internal data structures necessary for training\n",
        "word_vectors = model.wv   ## Gives the word vectors"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj9Y7PsAXJjG"
      },
      "source": [
        "#### Helper function to print"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Tku-zOXGK8"
      },
      "source": [
        "def print_most_similar(word_conf_pairs, k):\n",
        "    for i, (word, conf) in enumerate(word_conf_pairs):\n",
        "        print(\"{:.3f} {:s}\".format(conf, word))\n",
        "        if i >= k-1:\n",
        "            break\n",
        "    if k < len(word_conf_pairs):\n",
        "        print(\"...\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUJ2ZigzXvH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2cb59e0-fa1e-47e5-a9a1-c4d032ee2a46"
      },
      "source": [
        "print_most_similar(word_vectors.most_similar('king'),5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.741 queen\n",
            "0.735 prince\n",
            "0.703 kings\n",
            "0.697 throne\n",
            "0.681 regent\n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq-ugPPeW5IJ"
      },
      "source": [
        "#### Exercise 3\n",
        "In the class, we learned how to use the Word2Vec embeddings in Gensim. When the model is trained on the ‘text8’ dataset, give five most similar words to the word ‘tree’ using word2vec embedding trained on ‘text8’ dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbQnXKWFW98W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25059a88-73f3-4fc9-ac02-ef016f48edd7"
      },
      "source": [
        "## Write your code here\n",
        "\n",
        "print(model)\n",
        "word_vectors.most_similar('tree')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec(vocab=71290, size=100, alpha=0.025)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('leaf', 0.6883498430252075),\n",
              " ('bark', 0.6836422681808472),\n",
              " ('trees', 0.676968514919281),\n",
              " ('flower', 0.622661828994751),\n",
              " ('bird', 0.5972483158111572),\n",
              " ('avl', 0.5971020460128784),\n",
              " ('nest', 0.5871026515960693),\n",
              " ('fruit', 0.5820105075836182),\n",
              " ('cactus', 0.5814014077186584),\n",
              " ('beetle', 0.5785012245178223)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAjhxGApXji3"
      },
      "source": [
        "**Answer 3:**\n",
        "\n",
        "Five words most similar to the word **'tree'**: \n",
        "\n",
        "    'leaf',     0.6883498430252075\n",
        "    'bark',     0.6836422681808472\n",
        "    'trees',    0.676968514919281\n",
        "    'flower',   0.622661828994751\n",
        "    'bird',     0.5972483158111572\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVcvHhtkZCyU"
      },
      "source": [
        "## Word Arithmetics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKI_ojISoRJw"
      },
      "source": [
        "####Exercise 4\n",
        "With the Word2Vec model trained on text8 dataset, calculate the following:\n",
        "\n",
        "*\twoman + king - man = ?\n",
        "*\tchair + table - work = ?\n",
        "*\tQueens - queen + person = ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M35vzMIRZY0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06db24f5-236a-458e-9877-1a6ebcdbdb34"
      },
      "source": [
        "## Write your code here\n",
        "\n",
        "#Cosine Similarity\n",
        "\n",
        "print(model)\n",
        "print(dataset)\n",
        "\n",
        "print(\"woman + king - man =\")\n",
        "print_most_similar(word_vectors.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"]), 3)\n",
        "\n",
        "print(\"chair + table - work =\")\n",
        "print_most_similar(word_vectors.most_similar(positive=[\"chair\", \"table\"], negative=[\"work\"]), 3)\n",
        "\n",
        "print(\"queens - queen + person =\")\n",
        "print_most_similar(word_vectors.most_similar(negative=[\"queens\"], positive=[\"queen\", \"person\"]), 3)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec(vocab=71290, size=100, alpha=0.025)\n",
            "<text8.Dataset object at 0x7f8fe9035b50>\n",
            "woman + king - man =\n",
            "0.714 queen\n",
            "0.650 daughter\n",
            "0.647 empress\n",
            "...\n",
            "chair + table - work =\n",
            "0.600 bar\n",
            "0.530 chamber\n",
            "0.530 row\n",
            "...\n",
            "queens - queen + person =\n",
            "0.596 woman\n",
            "0.574 child\n",
            "0.557 victim\n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zLpG_LcZT4T"
      },
      "source": [
        "**Answer 4:**\n",
        "\n",
        "The 3 neighbouring words for the equation: \n",
        "\n",
        "    woman + king - man        =  queen (0.714)  /  daughter (0.650)  /  empress (0.647)\n",
        "\n",
        "    chair + table - work      =  bar (0.600)    /  chamber (0.530)  /   row (0.530)\n",
        "\n",
        "    queens - queen + person   =  woman (0.596)  /  child (0.574)   /    victim (0.557)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBBqBcUDZxMt"
      },
      "source": [
        "## Spam Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXlZhHnAlM7K"
      },
      "source": [
        "Some helper codes:\n",
        "* importing required modules\n",
        "* defining helper functions\n",
        "* Building model\n",
        "\n",
        "The code cells below are  hidden, that is by default you cannot see the code in them, but remember to run these cells. You can check the code by double clicking the cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWUieaCOZRx_"
      },
      "source": [
        "#@title\n",
        "# The modules needed to run the code\n",
        "import argparse  # To read commandline argument and parse it\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import os  # For file and directory handling\n",
        "import shutil  # For file and directory handling\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix  #For measuring performance\n",
        "\n",
        "# Some parameters\n",
        "DATA_DIR = \"data\"   # Data directory to save embedding\n",
        "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")  # Numpy file containing word embeddings\n",
        "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"  # Dataset URL from where data is downloaded\n",
        "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"  # The gensim embedding model we will use\n",
        "EMBEDDING_DIM = 300  # The embedding dimensions\n",
        "NUM_CLASSES = 2  # The number of classes in output-- Spam or Ham\n",
        "BATCH_SIZE = 128  # The batch size\n",
        "NUM_EPOCHS = 3  # number of epochs for which model is to be trained\n",
        "\n",
        "\n",
        "# data distribution is 4827 ham and 747 spam (total 5574), which \n",
        "# works out to approx 87% ham and 13% spam, so we take reciprocals\n",
        "# and this works out to being each spam (1) item as being approximately\n",
        "# 8 times as important as each ham (0) message.\n",
        "CLASS_WEIGHTS = { 0: 1, 1: 8 }  # To take care of imbalance in classes\n",
        "\n",
        "tf.random.set_seed(42)  # Set the seed for random number generation to be able to reproduce results. \n",
        "\n",
        "\n",
        "# Data downloading and data Processing\n",
        "\n",
        "\n",
        "\n",
        "def download_and_read(url):\n",
        "    \"\"\"\n",
        "    The function downloads the data from given url, splits it into Text and Labels\n",
        "    Uses tf.keras.utils.get_file() function to download the data from url--> function \n",
        "    downloads the data from the given url, extracts it from the zip file and place it in folder \"datasets\" \n",
        "    with the name specified in the first argument.\n",
        "    tf.keras.utils.get_file(\n",
        "    fname, origin, untar=False, md5_hash=None, file_hash=None,\n",
        "    cache_subdir='datasets', hash_algorithm='auto', extract=False,\n",
        "    archive_format='auto', cache_dir=None)\n",
        "\n",
        "    Arguments:\n",
        "    url: The url link of the dataset in zip format\n",
        "\n",
        "    Returns:\n",
        "    Two lists containing texts and respective labels\n",
        "\n",
        "    \"\"\"\n",
        "    local_file = url.split('/')[-1]  # split the file name (last string after '/') from url\n",
        "    p = tf.keras.utils.get_file(local_file, url, \n",
        "        extract=True, cache_dir=\".\")  #function to download the data from url to folder datasets with name given in local_file\n",
        "    labels, texts = [], []\n",
        "    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")  # define the path of the file from which to read data: datasets/SMSSpamCollection\n",
        "    with open(local_file, \"r\") as fin:\n",
        "        for line in fin:\n",
        "            label, text = line.strip().split('\\t')  # The labels and text are in one line separated by tab space.\n",
        "            labels.append(1 if label == \"spam\" else 0)\n",
        "            texts.append(text)\n",
        "    return texts, labels\n",
        "\n",
        "def build_embedding_matrix(sequences, word2idx, embedding_dim, \n",
        "        embedding_file):\n",
        "    \"\"\"\n",
        "    The function reads the dict word2idx (word --> number) and written the corresponding\n",
        "    word vector for each word as defined by the Embedding model\n",
        "\n",
        "    Arguments:\n",
        "    sequences: not needed, not used-- just there because to suport back support for TF1 book\n",
        "    word2idx: Dictionary  containing words in the text and their respective idx as given by tokenizer.\n",
        "    embedding_dim: The number of units for the embedding layer\n",
        "    embedding_file: The data file in which embeddings will be store for future use.\n",
        "\n",
        "    \"\"\"\n",
        "    if os.path.exists(embedding_file):  # Checks if the embedding file already exists- then it justs loads it in the memory\n",
        "        E = np.load(embedding_file)\n",
        "    else:  # Else it creates the embedding file using the model specified in EMBEDDING_MODEL\n",
        "        vocab_size = len(word2idx)  # The vocabulary size is number of unique words in the text\n",
        "        E = np.zeros((vocab_size, embedding_dim)) # Creates a variable to store embeddings\n",
        "        word_vectors = api.load(EMBEDDING_MODEL)  # Get the embeddings from Gensim\n",
        "        for word, idx in word2idx.items():\n",
        "            try:\n",
        "                E[idx] = word_vectors.word_vec(word)  # For each word it converts it to respective word vector and store in Embedding file\n",
        "            except KeyError:   # word not in embedding\n",
        "                pass\n",
        "            # except IndexError: # UNKs are mapped to seq over VOCAB_SIZE as well as 1\n",
        "            #     pass\n",
        "        np.save(embedding_file, E)  # The embeddings are saved in a file for future reference\n",
        "    return E"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCiIzfo5Zv4J"
      },
      "source": [
        "#@title\n",
        "class SpamClassifierModel(tf.keras.Model):  # The model is build using model API of Keras with tf.Keras.Model as the parent class. \n",
        "# The class inherits train, predict methods of the parent class.\n",
        "    def __init__(self, vocab_sz, embed_sz, input_length,\n",
        "            num_filters, kernel_sz, output_sz, \n",
        "            run_mode, embedding_weights, \n",
        "            **kwargs):\n",
        "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
        "        if run_mode == \"scratch\":  # Choose the embedding layer scratch means the weights wil be traned from scratch\n",
        "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
        "                embed_sz,\n",
        "                input_length=input_length,\n",
        "                trainable=True)\n",
        "        elif run_mode == \"vectorizer\":  # Vectorizer means we use the pre-trained weights--> Transfer Learning\n",
        "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
        "                embed_sz,\n",
        "                input_length=input_length,\n",
        "                weights=[embedding_weights],\n",
        "                trainable=False)\n",
        "        else:  # This is the fine tuning mode- we use pre-trained weights for the embedding layer and fine tune them. \n",
        "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
        "                embed_sz,\n",
        "                input_length=input_length,\n",
        "                weights=[embedding_weights],\n",
        "                trainable=True)\n",
        "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)  # Add droput layer to avoid overfotting. \n",
        "        self.conv = tf.keras.layers.Conv1D(filters=num_filters,  # Define the 1D convolutional layer \n",
        "            kernel_size=kernel_sz,\n",
        "            activation=\"relu\")\n",
        "        self.pool = tf.keras.layers.GlobalMaxPooling1D()  # The pooling layer\n",
        "        self.dense = tf.keras.layers.Dense(output_sz, \n",
        "            activation=\"softmax\")  # And the last classifying layer consists of a fully connected Dense layer\n",
        "\n",
        "    def call(self, x):  # This function performs forward pass in the model. \n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMNs8evikv8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8984717b-cf4b-407f-c160-eeed42135c6d"
      },
      "source": [
        "#@title\n",
        "# The code below requires a folder to be created\n",
        "!mkdir data\n",
        "\n",
        "## Now we will use the functions and model defined above --> ideally they should be done in a separate file-- main.py\n",
        "\n",
        "# read data\n",
        "texts, labels = download_and_read(DATASET_URL)\n",
        "\n",
        "# tokenize and pad text so that each text is of same size\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "text_sequences = tokenizer.texts_to_sequences(texts)\n",
        "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
        "num_records = len(text_sequences)\n",
        "max_seqlen = len(text_sequences[0])\n",
        "#print(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))\n",
        "\n",
        "# labels --> convert labels to categorical labels (one hot encoded)\n",
        "cat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)\n",
        "\n",
        "# vocabulary --> Create word mapping and its inverse\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {v:k for k, v in word2idx.items()}\n",
        "word2idx[\"PAD\"] = 0\n",
        "idx2word[0] = \"PAD\"\n",
        "vocab_size = len(word2idx)\n",
        "#print(\"vocab size: {:d}\".format(vocab_size))\n",
        "\n",
        "# load the dataset as tensors, split it into test, train and validation set\n",
        "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
        "dataset = dataset.shuffle(10000)\n",
        "test_size = num_records // 4\n",
        "val_size = (num_records - test_size) // 10\n",
        "test_dataset = dataset.take(test_size)\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\n",
        "train_dataset = dataset.skip(test_size + val_size)\n",
        "\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Build the embedding\n",
        "E = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM,\n",
        "    EMBEDDING_NUMPY_FILE)\n",
        "#print(\"Embedding matrix:\", E.shape)\n",
        "\n",
        "#Since we are not passing the mode by command line in this file we need to give a value to run_mode\n",
        "run_mode = 'scratch'\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "204800/203415 [==============================] - 1s 3us/step\n",
            "212992/203415 [===============================] - 1s 2us/step\n",
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MndykezKk56_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325aecd1-a3b7-4fe6-8b7f-b4d2c2770e6e"
      },
      "source": [
        "# Now we use the SpamClassifierModel class to create a model\n",
        "conv_num_filters = 256\n",
        "conv_kernel_size = 3\n",
        "model = SpamClassifierModel(\n",
        "    vocab_size, EMBEDDING_DIM, max_seqlen, \n",
        "    conv_num_filters, conv_kernel_size, NUM_CLASSES,\n",
        "    run_mode, E)\n",
        "model.build(input_shape=(None, max_seqlen))\n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"spam_classifier_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  2703000   \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  multiple                 0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " conv1d (Conv1D)             multiple                  230656    \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  multiple                 0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,934,170\n",
            "Trainable params: 2,934,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLeHPvIumV6q"
      },
      "source": [
        "#### Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWtsovxqmVAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af738af0-6006-4166-82a3-6eeeacc1de10"
      },
      "source": [
        "# Define  compile and train\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "# Now we train the model\n",
        "model.fit(train_dataset, epochs=NUM_EPOCHS, \n",
        "    validation_data=val_dataset,\n",
        "    class_weight=CLASS_WEIGHTS)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "29/29 [==============================] - 12s 25ms/step - loss: 0.9490 - accuracy: 0.6711 - val_loss: 0.2144 - val_accuracy: 0.9896\n",
            "Epoch 2/3\n",
            "29/29 [==============================] - 1s 21ms/step - loss: 0.2933 - accuracy: 0.9704 - val_loss: 0.0552 - val_accuracy: 0.9792\n",
            "Epoch 3/3\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 0.0915 - accuracy: 0.9898 - val_loss: 0.0539 - val_accuracy: 0.9818\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8fdb9fdb10>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Cc0hUMTnAX9"
      },
      "source": [
        "### And now we evaluate the model on test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr1dT-1UnBFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ca103d-ac9f-4e43-e9b3-f40b3af6b40e"
      },
      "source": [
        "# Lastly we evaluate the trained model against test set\n",
        "labels, predictions = [], []\n",
        "for Xtest, Ytest in test_dataset:  \n",
        "    Ytest_ = model.predict_on_batch(Xtest)  # for each test test predict the label\n",
        "    ytest = np.argmax(Ytest, axis=1)  # Get the label with highest probabilty from actual test output\n",
        "    ytest_ = np.argmax(Ytest_, axis=1) # Get the label with highest probabilty from predictted test output\n",
        "    labels.extend(ytest.tolist())  # add to list\n",
        "    predictions.extend(ytest.tolist())  # add to list\n",
        "\n",
        "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))  # Calculate accuracy score"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEah9aBlneEf"
      },
      "source": [
        "####Exercise 5\n",
        "In the spam classifier what is the false positive and false negative on the test dataset?  What does it tell you about the trained model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj69HAaZnToi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8850e149-5d4c-44b9-8f0a-ffcf5d12fca9"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "print(\"Confusion Matrix:\\n\")\n",
        "print(confusion_matrix(labels, predictions))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "\n",
            "[[1091    0]\n",
            " [   0  189]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBxQkUtv3E6U"
      },
      "source": [
        "**Answer 5:**\n",
        "\n",
        "**Confusion Matrix:**\n",
        "\n",
        "    Confusion Matrix:   [1091    0]\n",
        "                        [   0  189]\n",
        "\n",
        "    False Positive = 0\n",
        "    False Negative = 0\n",
        "\n",
        "**The spam classfier obtained a test accuracy of 100% with 0 False Positive and 0 False Negative. This indicates that the model is highly efficient. However, more exploration on larger databases should be considered in order to evaluate the model's performance.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9PiYvfro0TA"
      },
      "source": [
        "# Optional Exercise\n",
        "Consider the Tweet sentiment dataset from Stanford: http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip Which fields you will require for training? Modify the `download_and_read(url)` function to read from this URL and return the necessary labeled sentences.\n",
        "\n",
        "What is the accuracy of your twitter sentiment model after 5 epochs, after 10 epochs, after 15 epochs? Do you think it should be trained for more than 15 epochs? Give reasoning for your answer. \n",
        "\n",
        "In the class we used bi-LSTM, try change the recurrent network from bi-LSTM to GRU, how does it change the model performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X29UGobFo9Uw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}