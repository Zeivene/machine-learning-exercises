{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amita-kapoor/UO-Artificial-Intelligence-Cloud-and-Edge-Implementations/blob/master/Excercise_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZDq2nwZp2dv"
      },
      "source": [
        "## Classification Exercises\n",
        "\n",
        "For these exercises use the GPU in Google Colab. To enable GPU go to top menu bar in **EDIT** menu go to **NoteBook Settings**. Once you click it a window opens, in the hardware accelerator dropdown menu choose GPU. \n",
        "\n",
        "![alt](https://drive.google.com/uc?id=1rZf9pvb5rqY4rFwYqUhdmPkSrzaXBhPg)\n",
        "\n",
        "### Introduction\n",
        "\n",
        "We have already learned about Neural Networks and discussed Multilayered Perceptrons in depth. In this exercise, we will be testing our understanding of the underlying concepts with special emphasis to [Hyperparameter tuning](https://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568). \n",
        "\n",
        "After doing these exercises, you would be able to better understand:\n",
        "\n",
        "* The architecture of a neural network\n",
        "* The parameters (training) of a neural network and how they change with changing architecture.\n",
        "* Hyperparameter tuning: batch size, number of hidden units and optimizers.\n",
        "\n",
        "We encourage you to work with other hyperparameters as well like learning rate, number of layers, activation functions etc.  And in the end there is an optional exercise, where you can see if what you observe for the MNIST dataset is true for other dataset as well.\n",
        "\n",
        "The Notebook is divided in three parts: Building the Model, Reading the dataset and Hyperparameters. It contains five exercises in total and one additional optional exercise:\n",
        "\n",
        "* [Exercise 1](#ex_1)\n",
        "* [Exercise 2](#ex_2)\n",
        "* [Exercise 3](#ex_3)\n",
        "* [Exercise 4](#ex_4)\n",
        "* [Exercise 5](#ex_5)\n",
        "* [Optional Exercise](#ex_O)\n",
        "\n",
        "\n",
        "You have to do all the five exercises. Run the code given with each exercise and write down your answer just below each exercise. Wish you all the best.\n",
        "\n",
        "\n",
        "### Part 1: Building the model\n",
        "Below we define a function to built a neural network model using TensorFlow Keras. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "3mhZ0xlMRqO6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "def built_model(input_shape, n_hidden, nb_classes, optimizer='SGD'):\n",
        "  '''\n",
        "  The function builds a fully connected neural network with two hidden layers\n",
        "  Arguments:\n",
        "  input_shape: The number of inputs to the neural network\n",
        "  n_hidden: Number of hidden neurons in the hidden layers\n",
        "  nb_classes: Number of neurons in the output layer\n",
        "  optimizer: The optimizer used to train the model. \n",
        "  By default we use Stochastic Gradient Descent.\n",
        "  \n",
        "  Returns:\n",
        "  The function returns A model with loss and optimizer defined\n",
        "  '''  \n",
        "  model = tf.keras.models.Sequential()\n",
        "  ## First Hidden layer  \n",
        "  model.add(keras.layers.Dense(n_hidden,\n",
        "       input_shape=(input_shape,),\n",
        "       name='dense_layer', activation='relu'))\n",
        "    \n",
        "  ## Second Hidden Layer\n",
        "  model.add(keras.layers.Dense(n_hidden,\n",
        "        name='dense_layer_2', activation='relu'))\n",
        "    \n",
        "  ## Output Layer  \n",
        "  model.add(keras.layers.Dense(nb_classes,\n",
        "        name='dense_layer_3', activation='softmax'))\n",
        "    \n",
        "  ## Define loss and optimizer \n",
        "  model.compile(optimizer=optimizer, \n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5E0fCSDrwu_"
      },
      "source": [
        "<a id='ex_1'></a>\n",
        "**Exercise 1** What should be the values of the arguments `INPUT_SHAPE`: the number of input units, `N_HIDDEN`: the number of hidden units, and `NB_CLASSES`: the number of output units, if we want to build a model using `built_model` function with the specifications given in the figure:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1pcj2sHJK6CmhMjUo43AMNBxnU4ixQne3)\n",
        "\n",
        "\n",
        "\n",
        "To build this network we used TensorFlow Keras `plot_model` function available in `utils` model. You can learn more about the function from [TensorFlow docs](https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sBfRdV8ARuCe"
      },
      "outputs": [],
      "source": [
        "# Task to do\n",
        "INPUT_SHAPE = 5\n",
        "N_HIDDEN = 10\n",
        "NB_CLASSES = 2\n",
        "\n",
        "\n",
        "## Do not change anything below\n",
        "assert(INPUT_SHAPE == 5), \"Input shape incorrect\"\n",
        "assert(N_HIDDEN == 10), \"Number of hidden neurons incorrect\"\n",
        "assert(NB_CLASSES == 2), \"Number of output units incorrect\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "s00urUTcqfZR"
      },
      "outputs": [],
      "source": [
        "model = built_model(INPUT_SHAPE, N_HIDDEN,NB_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTccrPMNyZF5"
      },
      "source": [
        "<a id='ex_2'></a>\n",
        "**Exercise 2** Based on the input, hidden and output units what are the total number of trainable parameters in this model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DchkRcAyXM6",
        "outputId": "11d79511-a54e-4c4b-ca1d-db8467aa2f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters in the model are 192\n"
          ]
        }
      ],
      "source": [
        "# Task to do\n",
        "trainable_parameters = 192 # (5 * 10 + 10) + (10 * 10 + 10) + (10 * 2 + 2)\n",
        "\n",
        "## Do not change anything below\n",
        "assert trainable_parameters==model.count_params(), \"Your answer is incorrect\"\n",
        "print(\"Number of trainable parameters in the model are\", trainable_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g33IqZrGzLeW"
      },
      "source": [
        "Good work! Let us now visualize the summary of the model created. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4OteoBfx_QH",
        "outputId": "53d1d247-0c50-42fb-e4a8-963c3a7c04f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer (Dense)         (None, 10)                60        \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 10)                110       \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 22        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 192\n",
            "Trainable params: 192\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qeqEE5A3MQl"
      },
      "source": [
        "### Part 2: Reading the dataset\n",
        "\n",
        "We will continue with the MNIST dataset. \n",
        "\n",
        "###### Just run the cells in this part of the notebook. Do not change anything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "D3OEjwzLyBoO"
      },
      "outputs": [],
      "source": [
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "94uZnloL4-lB"
      },
      "outputs": [],
      "source": [
        "# Processing the data\n",
        "assert(len(X_train.shape)==3), \"The input data is not of the right shape\"\n",
        "RESHAPED = X_train.shape[1]*X_train.shape[2]\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJIMT6sT5Wtd",
        "outputId": "2f0d0a86-42c0-4e9c-d052-2efbd74432b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "# Data Normalization\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIRrNUTj4h_R"
      },
      "source": [
        "For the MNIST dataset the number of input and number of output units are fixed. However we can choose different values of hidden units. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "si1FflpL3_hj"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = RESHAPED\n",
        "NB_CLASSES = len(set(Y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ISwN58Q48a03"
      },
      "outputs": [],
      "source": [
        "# one-hot encode\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gjEx-hQ6Xsv"
      },
      "source": [
        "### Part 3: Hyperparameters\n",
        "\n",
        "<a id='ex_3'></a>\n",
        "**Exercise 3:** The aim of this exercise is to understand the affect of changing number of hidden units on the model performance. Change the number of hidden units, and train the model. Compare the model performance in terms of accuracy. What do you understand from this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWggLtWe7MUR"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Accuracy improves when the number of hidden units increases. However, to a certain point the accuracy are neglegible and improvements are minimal.\n",
        "\n",
        "**Test Accuracy from 50 Epochs:**\n",
        "\n",
        "    5 Hidden Layers:     87.93 %\n",
        "\n",
        "    10 Hidden Layers:    92.68 %\n",
        "\n",
        "    20 Hidden Layers:    94.50 %\n",
        "\n",
        "    50 Hidden Layers:    96.01 %\n",
        "\n",
        "    100 Hidden Layers:   96.25 %\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "qscy8jLk6BEA"
      },
      "outputs": [],
      "source": [
        "# Task to do choose different values for number of hidden units (minimum five different values)\n",
        "N_HIDDEN = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yG8aKH37krD",
        "outputId": "edf3a6a5-f1e9-498d-8dcb-9e2af39e255e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 1.4275 - accuracy: 0.6457 - val_loss: 0.7220 - val_accuracy: 0.8468\n",
            "Epoch 2/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.5814 - accuracy: 0.8547 - val_loss: 0.4433 - val_accuracy: 0.8863\n",
            "Epoch 3/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.4306 - accuracy: 0.8823 - val_loss: 0.3683 - val_accuracy: 0.8979\n",
            "Epoch 4/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3742 - accuracy: 0.8953 - val_loss: 0.3338 - val_accuracy: 0.9032\n",
            "Epoch 5/50\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.3432 - accuracy: 0.9026 - val_loss: 0.3096 - val_accuracy: 0.9089\n",
            "Epoch 6/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.3214 - accuracy: 0.9084 - val_loss: 0.2952 - val_accuracy: 0.9136\n",
            "Epoch 7/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.3053 - accuracy: 0.9123 - val_loss: 0.2816 - val_accuracy: 0.9187\n",
            "Epoch 8/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2916 - accuracy: 0.9161 - val_loss: 0.2722 - val_accuracy: 0.9224\n",
            "Epoch 9/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2799 - accuracy: 0.9192 - val_loss: 0.2626 - val_accuracy: 0.9249\n",
            "Epoch 10/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2693 - accuracy: 0.9223 - val_loss: 0.2537 - val_accuracy: 0.9281\n",
            "Epoch 11/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2602 - accuracy: 0.9256 - val_loss: 0.2460 - val_accuracy: 0.9299\n",
            "Epoch 12/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2514 - accuracy: 0.9274 - val_loss: 0.2398 - val_accuracy: 0.9322\n",
            "Epoch 13/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2433 - accuracy: 0.9296 - val_loss: 0.2320 - val_accuracy: 0.9334\n",
            "Epoch 14/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2358 - accuracy: 0.9319 - val_loss: 0.2260 - val_accuracy: 0.9362\n",
            "Epoch 15/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2288 - accuracy: 0.9340 - val_loss: 0.2220 - val_accuracy: 0.9377\n",
            "Epoch 16/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2222 - accuracy: 0.9360 - val_loss: 0.2158 - val_accuracy: 0.9387\n",
            "Epoch 17/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2160 - accuracy: 0.9376 - val_loss: 0.2111 - val_accuracy: 0.9400\n",
            "Epoch 18/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2102 - accuracy: 0.9398 - val_loss: 0.2054 - val_accuracy: 0.9426\n",
            "Epoch 19/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2047 - accuracy: 0.9412 - val_loss: 0.2017 - val_accuracy: 0.9431\n",
            "Epoch 20/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1994 - accuracy: 0.9422 - val_loss: 0.1972 - val_accuracy: 0.9442\n",
            "Epoch 21/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1943 - accuracy: 0.9442 - val_loss: 0.1947 - val_accuracy: 0.9448\n",
            "Epoch 22/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1896 - accuracy: 0.9455 - val_loss: 0.1912 - val_accuracy: 0.9466\n",
            "Epoch 23/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1851 - accuracy: 0.9471 - val_loss: 0.1854 - val_accuracy: 0.9475\n",
            "Epoch 24/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.9482 - val_loss: 0.1824 - val_accuracy: 0.9496\n",
            "Epoch 25/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.9493 - val_loss: 0.1795 - val_accuracy: 0.9494\n",
            "Epoch 26/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1728 - accuracy: 0.9496 - val_loss: 0.1764 - val_accuracy: 0.9505\n",
            "Epoch 27/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1690 - accuracy: 0.9515 - val_loss: 0.1736 - val_accuracy: 0.9520\n",
            "Epoch 28/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1652 - accuracy: 0.9526 - val_loss: 0.1721 - val_accuracy: 0.9528\n",
            "Epoch 29/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1618 - accuracy: 0.9535 - val_loss: 0.1678 - val_accuracy: 0.9534\n",
            "Epoch 30/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1584 - accuracy: 0.9540 - val_loss: 0.1657 - val_accuracy: 0.9531\n",
            "Epoch 31/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1549 - accuracy: 0.9559 - val_loss: 0.1626 - val_accuracy: 0.9555\n",
            "Epoch 32/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1521 - accuracy: 0.9561 - val_loss: 0.1603 - val_accuracy: 0.9548\n",
            "Epoch 33/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1488 - accuracy: 0.9573 - val_loss: 0.1585 - val_accuracy: 0.9562\n",
            "Epoch 34/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1459 - accuracy: 0.9582 - val_loss: 0.1558 - val_accuracy: 0.9564\n",
            "Epoch 35/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1431 - accuracy: 0.9591 - val_loss: 0.1537 - val_accuracy: 0.9573\n",
            "Epoch 36/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1403 - accuracy: 0.9596 - val_loss: 0.1516 - val_accuracy: 0.9574\n",
            "Epoch 37/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1375 - accuracy: 0.9607 - val_loss: 0.1504 - val_accuracy: 0.9587\n",
            "Epoch 38/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1349 - accuracy: 0.9620 - val_loss: 0.1479 - val_accuracy: 0.9582\n",
            "Epoch 39/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1325 - accuracy: 0.9625 - val_loss: 0.1457 - val_accuracy: 0.9597\n",
            "Epoch 40/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1297 - accuracy: 0.9630 - val_loss: 0.1450 - val_accuracy: 0.9599\n",
            "Epoch 41/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1274 - accuracy: 0.9632 - val_loss: 0.1431 - val_accuracy: 0.9590\n",
            "Epoch 42/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1252 - accuracy: 0.9643 - val_loss: 0.1416 - val_accuracy: 0.9609\n",
            "Epoch 43/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1231 - accuracy: 0.9655 - val_loss: 0.1390 - val_accuracy: 0.9613\n",
            "Epoch 44/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1206 - accuracy: 0.9662 - val_loss: 0.1376 - val_accuracy: 0.9610\n",
            "Epoch 45/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1188 - accuracy: 0.9665 - val_loss: 0.1361 - val_accuracy: 0.9619\n",
            "Epoch 46/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1165 - accuracy: 0.9670 - val_loss: 0.1352 - val_accuracy: 0.9618\n",
            "Epoch 47/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1147 - accuracy: 0.9679 - val_loss: 0.1336 - val_accuracy: 0.9632\n",
            "Epoch 48/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1125 - accuracy: 0.9684 - val_loss: 0.1314 - val_accuracy: 0.9628\n",
            "Epoch 49/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1108 - accuracy: 0.9688 - val_loss: 0.1304 - val_accuracy: 0.9629\n",
            "Epoch 50/50\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1088 - accuracy: 0.9690 - val_loss: 0.1298 - val_accuracy: 0.9631\n",
            "313/313 [==============================] - 0s 880us/step - loss: 0.1252 - accuracy: 0.9625\n",
            "Test accuracy: 96.25 %\n"
          ]
        }
      ],
      "source": [
        "## Do not change anything below\n",
        "model = built_model(INPUT_SHAPE,N_HIDDEN, NB_CLASSES)\n",
        "history = model.fit(X_train, Y_train,\n",
        "\t\tbatch_size=128, epochs=50,\n",
        "\t\tverbose=1, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: {:.2f} %'.format(test_acc*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqfMXnlJA02g"
      },
      "source": [
        "<a id='ex_4'></a>\n",
        "**Exercise 4:** Let us now repeat the same after changing the batch size (minimum 5 different values). Compare the model performance in terms of accuracy. What do you understand from this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z0TctxjB0KA"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Based from a sample size of 48,000. As batch size increases, accuracy diminishes due to poor generalisation. Larger batch size takes less time to train and are less accurate. Smaller batch size takes longer to train but yields better results.\n",
        "\n",
        "Furthermore, the smaller the batch size, the longer it takes to run due to additional iterations required from utilising extra batches of the samples, and vice versa.\n",
        "\n",
        "**Batch Sizes from 50 Epochs (Test Accuracy):**\n",
        "\n",
        "    10 Batch Size:                  97.77 %\n",
        "\n",
        "    50 Batch Size:                  97.54 %\n",
        "\n",
        "    100 Batch Size:                 96.79 %\n",
        "\n",
        "    200 Batch Size:                 95.78 %\n",
        "\n",
        "    480 Batch Size:                 93.49 %\n",
        "\n",
        "    4,800 Batch Size:               91.95 %\n",
        "\n",
        "    9,600 Batch Size:               77.33 %\n",
        "\n",
        "    48,000 Batch Size:              41.97 %\n",
        "\n",
        "    48,000 Batch Size (2nd Test):   33.05 %\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "TXO0QZtS_mQ_"
      },
      "outputs": [],
      "source": [
        "# Task to do choose different values for batch size (minimum five different values)\n",
        "BATCH_SIZE = 48000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1CVvMthBmWr",
        "outputId": "ad98d18f-0efa-42fb-901f-e1371c683a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 1s 802ms/step - loss: 2.3661 - accuracy: 0.1019 - val_loss: 2.3563 - val_accuracy: 0.1133\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 2.3595 - accuracy: 0.1035 - val_loss: 2.3498 - val_accuracy: 0.1147\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 2.3530 - accuracy: 0.1059 - val_loss: 2.3436 - val_accuracy: 0.1162\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 2.3468 - accuracy: 0.1082 - val_loss: 2.3374 - val_accuracy: 0.1179\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 2.3406 - accuracy: 0.1102 - val_loss: 2.3314 - val_accuracy: 0.1196\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 2.3346 - accuracy: 0.1120 - val_loss: 2.3256 - val_accuracy: 0.1228\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 2.3288 - accuracy: 0.1144 - val_loss: 2.3198 - val_accuracy: 0.1252\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 2.3230 - accuracy: 0.1166 - val_loss: 2.3142 - val_accuracy: 0.1272\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 394ms/step - loss: 2.3174 - accuracy: 0.1188 - val_loss: 2.3086 - val_accuracy: 0.1287\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 2.3119 - accuracy: 0.1211 - val_loss: 2.3032 - val_accuracy: 0.1309\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 394ms/step - loss: 2.3064 - accuracy: 0.1234 - val_loss: 2.2978 - val_accuracy: 0.1332\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 2.3011 - accuracy: 0.1255 - val_loss: 2.2925 - val_accuracy: 0.1353\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 2.2959 - accuracy: 0.1277 - val_loss: 2.2873 - val_accuracy: 0.1374\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 397ms/step - loss: 2.2907 - accuracy: 0.1302 - val_loss: 2.2822 - val_accuracy: 0.1398\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 2.2856 - accuracy: 0.1324 - val_loss: 2.2771 - val_accuracy: 0.1423\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 394ms/step - loss: 2.2806 - accuracy: 0.1343 - val_loss: 2.2721 - val_accuracy: 0.1447\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 391ms/step - loss: 2.2756 - accuracy: 0.1368 - val_loss: 2.2672 - val_accuracy: 0.1477\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 2.2707 - accuracy: 0.1394 - val_loss: 2.2623 - val_accuracy: 0.1500\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 2.2659 - accuracy: 0.1417 - val_loss: 2.2575 - val_accuracy: 0.1526\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 395ms/step - loss: 2.2611 - accuracy: 0.1441 - val_loss: 2.2527 - val_accuracy: 0.1567\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 2.2563 - accuracy: 0.1477 - val_loss: 2.2479 - val_accuracy: 0.1601\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 2.2516 - accuracy: 0.1509 - val_loss: 2.2432 - val_accuracy: 0.1643\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 2.2469 - accuracy: 0.1542 - val_loss: 2.2385 - val_accuracy: 0.1673\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 2.2423 - accuracy: 0.1578 - val_loss: 2.2339 - val_accuracy: 0.1711\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 2.2377 - accuracy: 0.1617 - val_loss: 2.2292 - val_accuracy: 0.1758\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 2.2331 - accuracy: 0.1660 - val_loss: 2.2246 - val_accuracy: 0.1807\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 397ms/step - loss: 2.2286 - accuracy: 0.1710 - val_loss: 2.2201 - val_accuracy: 0.1857\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 2.2240 - accuracy: 0.1756 - val_loss: 2.2155 - val_accuracy: 0.1916\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 2.2195 - accuracy: 0.1804 - val_loss: 2.2110 - val_accuracy: 0.1980\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 406ms/step - loss: 2.2150 - accuracy: 0.1858 - val_loss: 2.2064 - val_accuracy: 0.2021\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 392ms/step - loss: 2.2106 - accuracy: 0.1919 - val_loss: 2.2019 - val_accuracy: 0.2080\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 2.2061 - accuracy: 0.1975 - val_loss: 2.1974 - val_accuracy: 0.2138\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 2.2017 - accuracy: 0.2039 - val_loss: 2.1929 - val_accuracy: 0.2198\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 2.1972 - accuracy: 0.2100 - val_loss: 2.1884 - val_accuracy: 0.2251\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 2.1928 - accuracy: 0.2158 - val_loss: 2.1839 - val_accuracy: 0.2303\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 397ms/step - loss: 2.1884 - accuracy: 0.2229 - val_loss: 2.1795 - val_accuracy: 0.2363\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 392ms/step - loss: 2.1839 - accuracy: 0.2291 - val_loss: 2.1750 - val_accuracy: 0.2418\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 2.1795 - accuracy: 0.2363 - val_loss: 2.1705 - val_accuracy: 0.2482\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 2.1751 - accuracy: 0.2430 - val_loss: 2.1660 - val_accuracy: 0.2560\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 2.1706 - accuracy: 0.2508 - val_loss: 2.1615 - val_accuracy: 0.2641\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 2.1662 - accuracy: 0.2580 - val_loss: 2.1570 - val_accuracy: 0.2709\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 395ms/step - loss: 2.1618 - accuracy: 0.2659 - val_loss: 2.1525 - val_accuracy: 0.2780\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 406ms/step - loss: 2.1573 - accuracy: 0.2728 - val_loss: 2.1480 - val_accuracy: 0.2862\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 2.1529 - accuracy: 0.2797 - val_loss: 2.1435 - val_accuracy: 0.2946\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 2.1484 - accuracy: 0.2870 - val_loss: 2.1390 - val_accuracy: 0.3015\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 2.1440 - accuracy: 0.2943 - val_loss: 2.1344 - val_accuracy: 0.3077\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 2.1395 - accuracy: 0.3014 - val_loss: 2.1299 - val_accuracy: 0.3144\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 2.1350 - accuracy: 0.3084 - val_loss: 2.1253 - val_accuracy: 0.3214\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 2.1305 - accuracy: 0.3153 - val_loss: 2.1207 - val_accuracy: 0.3274\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 387ms/step - loss: 2.1260 - accuracy: 0.3227 - val_loss: 2.1161 - val_accuracy: 0.3351\n",
            "313/313 [==============================] - 0s 887us/step - loss: 2.1167 - accuracy: 0.3305\n",
            "Test accuracy: 33.05 %\n"
          ]
        }
      ],
      "source": [
        "## Do not change anything below\n",
        "model = built_model(INPUT_SHAPE,128, NB_CLASSES)\n",
        "history = model.fit(X_train, Y_train,\n",
        "\t\tbatch_size=BATCH_SIZE, epochs=50,\n",
        "\t\tverbose=1, validation_split=0.2)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: {:.2f} %'.format(test_acc*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L802oqkCBg8"
      },
      "source": [
        "<a id='ex_5'></a>\n",
        "**Exercise 5:** And now we do the same with different [optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) available in TensorFlow. Change the optimizers and compare the model performance in terms of accuracy. What do you understand from this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M0Dx7O2CEW8"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Performance of **Adam, Adamax, Nadam, RMSprop and SGD** yields similar results.\n",
        "\n",
        "**Adagrad** is slightly less accurate but above ***90%***.\n",
        "\n",
        "**Adadelta** ranges between ***80%*** to ***90%*** accuracy.\n",
        "\n",
        "The worst optimizer is **Ftrl**, producing a ***11.35%*** accuracy in this instance. In the notebook, it is explained that the algorithm is mostly suitable for shallow models with large and sparse feature space, which may not be appropriate for this dataset.\n",
        "\n",
        "**Test Accuracy based on 8 Different Optimizers:**\n",
        "\n",
        "    Adadelta:  86.37 %        # learning_rate=0.001\n",
        "\n",
        "    Adagrad:   92.75 %        # learning_rate=0.001\n",
        "\n",
        "    Adam:      98.21 %        # learning_rate=0.001\n",
        "\n",
        "    Adamax:    97.78 %        # learning_rate=0.001\n",
        "\n",
        "    Ftrl:      11.35 %        # learning_rate=0.001\n",
        "\n",
        "    Nadam:     97.57 %        # learning_rate=0.001\n",
        "\n",
        "    RMSprop:   97.77 %        # learning_rate=0.001\n",
        "\n",
        "    SGD:       96.58 %        # learning_rate=0.01\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "2DsZiC8CB7j5"
      },
      "outputs": [],
      "source": [
        "# Task to do choose different optimizers\n",
        "opt = tf.keras.optimizers.SGD(\n",
        "    learning_rate=0.01,\n",
        "    momentum=0.0,\n",
        "    nesterov=False,\n",
        "    name='SGD'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkcTxGl4CqK5",
        "outputId": "1666ac3c-76af-4bd6-8db4-9cb977e4b2eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 1.4843 - accuracy: 0.6285 - val_loss: 0.7624 - val_accuracy: 0.8405\n",
            "Epoch 2/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.6006 - accuracy: 0.8508 - val_loss: 0.4504 - val_accuracy: 0.8814\n",
            "Epoch 3/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.4348 - accuracy: 0.8816 - val_loss: 0.3685 - val_accuracy: 0.8981\n",
            "Epoch 4/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3739 - accuracy: 0.8949 - val_loss: 0.3304 - val_accuracy: 0.9060\n",
            "Epoch 5/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3397 - accuracy: 0.9033 - val_loss: 0.3056 - val_accuracy: 0.9117\n",
            "Epoch 6/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3163 - accuracy: 0.9092 - val_loss: 0.2883 - val_accuracy: 0.9172\n",
            "Epoch 7/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2986 - accuracy: 0.9138 - val_loss: 0.2726 - val_accuracy: 0.9232\n",
            "Epoch 8/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2839 - accuracy: 0.9179 - val_loss: 0.2619 - val_accuracy: 0.9252\n",
            "Epoch 9/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2716 - accuracy: 0.9219 - val_loss: 0.2509 - val_accuracy: 0.9275\n",
            "Epoch 10/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2603 - accuracy: 0.9249 - val_loss: 0.2434 - val_accuracy: 0.9302\n",
            "Epoch 11/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2510 - accuracy: 0.9278 - val_loss: 0.2351 - val_accuracy: 0.9318\n",
            "Epoch 12/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2420 - accuracy: 0.9302 - val_loss: 0.2275 - val_accuracy: 0.9340\n",
            "Epoch 13/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2338 - accuracy: 0.9329 - val_loss: 0.2224 - val_accuracy: 0.9351\n",
            "Epoch 14/50\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.2267 - accuracy: 0.9348 - val_loss: 0.2153 - val_accuracy: 0.9384\n",
            "Epoch 15/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2196 - accuracy: 0.9367 - val_loss: 0.2093 - val_accuracy: 0.9405\n",
            "Epoch 16/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2129 - accuracy: 0.9389 - val_loss: 0.2050 - val_accuracy: 0.9415\n",
            "Epoch 17/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2065 - accuracy: 0.9409 - val_loss: 0.1994 - val_accuracy: 0.9433\n",
            "Epoch 18/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2008 - accuracy: 0.9426 - val_loss: 0.1942 - val_accuracy: 0.9451\n",
            "Epoch 19/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1948 - accuracy: 0.9440 - val_loss: 0.1904 - val_accuracy: 0.9463\n",
            "Epoch 20/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1896 - accuracy: 0.9453 - val_loss: 0.1860 - val_accuracy: 0.9482\n",
            "Epoch 21/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1844 - accuracy: 0.9466 - val_loss: 0.1813 - val_accuracy: 0.9498\n",
            "Epoch 22/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1793 - accuracy: 0.9487 - val_loss: 0.1794 - val_accuracy: 0.9492\n",
            "Epoch 23/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1747 - accuracy: 0.9499 - val_loss: 0.1746 - val_accuracy: 0.9507\n",
            "Epoch 24/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1703 - accuracy: 0.9510 - val_loss: 0.1712 - val_accuracy: 0.9522\n",
            "Epoch 25/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1660 - accuracy: 0.9524 - val_loss: 0.1685 - val_accuracy: 0.9532\n",
            "Epoch 26/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1619 - accuracy: 0.9538 - val_loss: 0.1649 - val_accuracy: 0.9544\n",
            "Epoch 27/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1578 - accuracy: 0.9550 - val_loss: 0.1623 - val_accuracy: 0.9540\n",
            "Epoch 28/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1541 - accuracy: 0.9558 - val_loss: 0.1589 - val_accuracy: 0.9557\n",
            "Epoch 29/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1505 - accuracy: 0.9567 - val_loss: 0.1557 - val_accuracy: 0.9567\n",
            "Epoch 30/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1471 - accuracy: 0.9582 - val_loss: 0.1536 - val_accuracy: 0.9570\n",
            "Epoch 31/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1434 - accuracy: 0.9593 - val_loss: 0.1530 - val_accuracy: 0.9568\n",
            "Epoch 32/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1402 - accuracy: 0.9603 - val_loss: 0.1498 - val_accuracy: 0.9584\n",
            "Epoch 33/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1371 - accuracy: 0.9610 - val_loss: 0.1464 - val_accuracy: 0.9592\n",
            "Epoch 34/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1343 - accuracy: 0.9621 - val_loss: 0.1442 - val_accuracy: 0.9597\n",
            "Epoch 35/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1314 - accuracy: 0.9635 - val_loss: 0.1422 - val_accuracy: 0.9607\n",
            "Epoch 36/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1285 - accuracy: 0.9637 - val_loss: 0.1399 - val_accuracy: 0.9613\n",
            "Epoch 37/50\n",
            "375/375 [==============================] - 1s 4ms/step - loss: 0.1257 - accuracy: 0.9650 - val_loss: 0.1378 - val_accuracy: 0.9621\n",
            "Epoch 38/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1231 - accuracy: 0.9655 - val_loss: 0.1358 - val_accuracy: 0.9622\n",
            "Epoch 39/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1206 - accuracy: 0.9668 - val_loss: 0.1350 - val_accuracy: 0.9628\n",
            "Epoch 40/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1183 - accuracy: 0.9668 - val_loss: 0.1326 - val_accuracy: 0.9638\n",
            "Epoch 41/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1159 - accuracy: 0.9679 - val_loss: 0.1313 - val_accuracy: 0.9637\n",
            "Epoch 42/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1138 - accuracy: 0.9684 - val_loss: 0.1298 - val_accuracy: 0.9643\n",
            "Epoch 43/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1116 - accuracy: 0.9688 - val_loss: 0.1280 - val_accuracy: 0.9647\n",
            "Epoch 44/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1094 - accuracy: 0.9696 - val_loss: 0.1269 - val_accuracy: 0.9647\n",
            "Epoch 45/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1073 - accuracy: 0.9705 - val_loss: 0.1250 - val_accuracy: 0.9659\n",
            "Epoch 46/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1054 - accuracy: 0.9708 - val_loss: 0.1247 - val_accuracy: 0.9652\n",
            "Epoch 47/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1036 - accuracy: 0.9711 - val_loss: 0.1230 - val_accuracy: 0.9663\n",
            "Epoch 48/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1017 - accuracy: 0.9719 - val_loss: 0.1218 - val_accuracy: 0.9664\n",
            "Epoch 49/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0998 - accuracy: 0.9726 - val_loss: 0.1203 - val_accuracy: 0.9665\n",
            "Epoch 50/50\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0983 - accuracy: 0.9727 - val_loss: 0.1192 - val_accuracy: 0.9664\n",
            "313/313 [==============================] - 0s 904us/step - loss: 0.1152 - accuracy: 0.9658\n",
            "Test accuracy: 96.58 %\n"
          ]
        }
      ],
      "source": [
        "## Do not change anything below\n",
        "N_HIDDEN = 128\n",
        "model = built_model(INPUT_SHAPE,N_HIDDEN, NB_CLASSES, opt)\n",
        "history = model.fit(X_train, Y_train,\n",
        "\t\tbatch_size=128, epochs=50,\n",
        "\t\tverbose=1, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy: {:.2f} %'.format(test_acc*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-46TwuMnDdIB"
      },
      "source": [
        "<a id='ex_O'></a>\n",
        "### Optional Exercise: Fashion MNIST\n",
        "\n",
        "Repeat the above exercises (3-5) with different dataset. You can use Fashion MNIST another popular ML dataset. Are the results same? Comment.\n",
        "\n",
        "To download fashion mnist you can use the following code:\n",
        "\n",
        "```\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqc0ID8XDLCN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Excercise_MLP_Classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}